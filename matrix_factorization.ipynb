{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Matrix Factorization\n",
    "\n",
    "$R = UV'$ with gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#configure plotting\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "import matplotlib;matplotlib.rcParams['figure.figsize'] = (8,5)\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(943, 1682)\n"
     ]
    }
   ],
   "source": [
    "from movieLensAnalyzer import MovieLensAnalyzer \n",
    "movieLensAnalyzer = MovieLensAnalyzer()\n",
    "# Returns as a numpy array\n",
    "userMovieRatingMatrix = movieLensAnalyzer.getUserMovieRatingMatrix()\n",
    "print(userMovieRatingMatrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "# OLD\n",
    "\n",
    "R_    = np.array([[1,3,3,4,5],\n",
    "                  [1,2,4,3,5],\n",
    "                  [5,3,2,5,1]])\n",
    "mask_ = np.array([[1,1,1,1,1],\n",
    "                  [1,0,0,0,0],\n",
    "                  [1,1,1,1,1]])\n",
    "\n",
    "# NEW\n",
    "R_ = userMovieRatingMatrix\n",
    "# random boolean mask for which values will be changed\n",
    "# Below one liner only works for 50 percent\n",
    "# mask_ = np.random.randint(0,2,size=userMovieRatingMatrix.shape) # Random integer 0 to 1 for 50% of it\n",
    "\n",
    "# Hard code own percentagezero version to set random ones to 0 from mask\n",
    "# TODO: Use more efficient numpy statements instead of for loops\n",
    "percentageZero = 0.3 # Set 30 percent of data to be 0\n",
    "mask_ = np.ones(userMovieRatingMatrix.shape)\n",
    "# mask_ = np.ones((3,3))\n",
    "for i in range(mask_.shape[0]):\n",
    "    for j in range(mask_.shape[1]):\n",
    "        if (random.random() < percentageZero):\n",
    "            mask_[i][j] = 0\n",
    "\n",
    "n_users, n_items = R_.shape\n",
    "latent_dim = 2\n",
    "\n",
    "R = tf.placeholder(tf.float32)\n",
    "mask = tf.placeholder(tf.float32)\n",
    "\n",
    "U = tf.Variable(tf.random_uniform([n_users, latent_dim], -1.0, 1.0), name='user_matrix')\n",
    "V = tf.Variable(tf.random_uniform([n_items, latent_dim], -1.0, 1.0), name='item_matrix')\n",
    "\n",
    "feed_dict = {\n",
    "    R: R_,\n",
    "    mask: mask_\n",
    "}\n",
    "\n",
    "reg_rate = 0.1\n",
    "learning_rate = 0.01\n",
    "\n",
    "reg = tf.norm(U) + tf.norm(V)\n",
    "error = tf.reduce_sum(tf.multiply(mask, tf.square(R - tf.matmul(U, V, transpose_b=True))))\n",
    "loss = error + reg_rate * reg\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "train = optimizer.minimize(loss)\n",
    "\n",
    "initialize = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Good way to quickly check your work as you go.\n",
    "# with tf.Session() as sess:\n",
    "#     sess.run(initialize)\n",
    "#     print(sess.run(loss, feed_dict))\n",
    "#     print(sess.graph.get_tensor_by_name('user_matrix:0').eval(session=sess))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train 1 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model():\n",
    "    losses = []\n",
    "    R_hat = None\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(initialize)\n",
    "        #for i in range(100):\n",
    "        for i in range(20):\n",
    "            sess.run(train, feed_dict)\n",
    "            losses.append(sess.run(loss, feed_dict))\n",
    "\n",
    "        R_hat = sess.run(tf.matmul(U, V, transpose_b=True), feed_dict)\n",
    "    return R_hat, losses\n",
    "R_hat, losses = train_model()\n",
    "print(\"Final loss:\", losses[-1])\n",
    "print(\"Original:\")\n",
    "print(R_)\n",
    "print(\"Reconstruction:\")\n",
    "print(R_hat)\n",
    "plt.plot(losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uncertainty(?) (warning: SLOW)\n",
    "\n",
    "Not sure if this is the right way to get uncertainty (by just fitting this model over and over)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "final_R_hats = []\n",
    "#for trial in range(100):\n",
    "for trial in range(2):\n",
    "    if trial % 10 == 0: print(trial)\n",
    "    R_hat, _ = train_model()\n",
    "    final_R_hats.append(R_hat)\n",
    "final_R_hats = np.array(final_R_hats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Visualize each value's uncertainty\n",
    "f, axes = plt.subplots(n_users, n_items, sharex=True, sharey=True)\n",
    "for i in range(n_users):\n",
    "    for j in range(n_items):\n",
    "        axes[i][j].hist(final_R_hats[:,i,j])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "\n",
    "- The \"uncertainty\" estimates are bad. In fact, I weren't able to even get good predictions by adjusting regularization. I wonder if the data is just too small.\n",
    "- How to constrain it so ratings are positive? Maybe use Exponentiated Loss?\n",
    "- Is there a better way to estimate uncertainty?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
