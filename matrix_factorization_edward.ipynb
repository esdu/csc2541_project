{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Matrix Factorization with Edward\n",
    "\n",
    "$R = UV'$ with VI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#configure plotting\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "import matplotlib;matplotlib.rcParams['figure.figsize'] = (8,5)\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import edward as ed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R_    = np.array([[1,3,3,4,5],\n",
    "                  [1,2,4,3,5],\n",
    "                  [5,3,2,5,1]])\n",
    "mask_ = np.array([[1,1,1,1,1],\n",
    "                  [1,0,0,0,0],\n",
    "                  [1,1,1,1,1]])\n",
    "\n",
    "n_users, n_items = R_.shape\n",
    "latent_dim = 2\n",
    "\n",
    "mask = tf.placeholder(tf.float32, shape=mask_.shape)\n",
    "\n",
    "U = ed.models.Normal(loc=tf.zeros([n_users, latent_dim]), scale=tf.ones([n_users, latent_dim]), name=\"user_matrix\")\n",
    "V = ed.models.Normal(loc=tf.zeros([n_items, latent_dim]), scale=tf.ones([n_items, latent_dim]), name=\"item_matrix\")\n",
    "R = ed.models.Normal(loc=tf.matmul(U, V, transpose_b=True), scale=100 * (1-mask) + 0.1)\n",
    "\n",
    "qU = ed.models.Normal(loc=tf.Variable(tf.random_normal([n_users, latent_dim])),\n",
    "                      scale=tf.nn.softplus(tf.Variable(tf.random_normal([n_users, latent_dim]))))\n",
    "qV = ed.models.Normal(loc=tf.Variable(tf.random_normal([n_items, latent_dim])),\n",
    "                      scale=tf.nn.softplus(tf.Variable(tf.random_normal([n_items, latent_dim]))))\n",
    "\n",
    "sess = ed.get_session()\n",
    "tf.global_variables_initializer().run()\n",
    "\n",
    "rhat_samples = tf.stack([tf.matmul(qU.sample(), qV.sample(), transpose_b=True) for _ in range(100)])\n",
    "\n",
    "def plot_R(rhat_samples, title=''):\n",
    "    rhats = rhat_samples.eval()\n",
    "    f, axes = plt.subplots(n_users, n_items, sharex=True, sharey=True)\n",
    "    for i in range(n_users):\n",
    "        for j in range(n_items):\n",
    "            axes[i][j].axvline(x=R_[i,j], color='r')\n",
    "            axes[i][j].hist(rhats[:,i,j])\n",
    "    f.suptitle(title)\n",
    "    plt.show()\n",
    "\n",
    "plot_R(rhat_samples, 'Prior')\n",
    "inference = ed.KLqp({U: qU, V: qV}, data={R: R_, mask: mask_})\n",
    "inference.run(n_iter=1000, n_samples=5)\n",
    "plot_R(rhat_samples, 'Posterior')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "\n",
    "- Why are posterior uncertainty bounds so tight?\n",
    "- Better priors / prior distributions?\n",
    "- How to regularize this?\n",
    "- How to update this when we see new data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
