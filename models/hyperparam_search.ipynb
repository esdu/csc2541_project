{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib; matplotlib.rcParams['figure.figsize'] = (15,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Reload all packages - make debugging easier\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from nnmf_svi_eddie import save_graph_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nnmf_svi_eddie import NNMF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is just for debugging. Use `hyperparam_search.py` instead.\n",
    "\n",
    "```\n",
    "[x] Split to train/validation\n",
    "[x] MSE\n",
    "[x] Save weights\n",
    "[ ] Visualize results\n",
    "\n",
    "Next: Find out what dataset to train on, and test MSE on validation set.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hypers_config(seed):\n",
    "    import random\n",
    "    random.seed(seed)\n",
    "    \n",
    "    D = random.randint(5,100)\n",
    "    Dp = random.randint(5,100)\n",
    "    \n",
    "    nn_hidden_layer_dims = []\n",
    "    n_layers = random.randint(1,5)\n",
    "    for l in range(n_layers):\n",
    "        nn_hidden_layer_dims.append(random.randint(5,100))\n",
    "        \n",
    "    batch_size = random.randint(100,400)\n",
    "    n_samples = random.choice([0,10,100])\n",
    "    \n",
    "    pZ_prior_stddev = random.randrange(200) / 100 # 0 to 2\n",
    "    pR_stddev = random.randrange(200) / 100 # 0 to 2\n",
    "    \n",
    "    nn_W_init_mean = 0.\n",
    "    nn_W_init_stddev = random.randrange(200) / 100 # 0 to 2\n",
    "    nn_b_init_mean = 0.\n",
    "    nn_b_init_stddev = random.randrange(200) / 100 # 0 to 2\n",
    "\n",
    "    optimizer = 'adam'\n",
    "    lr_init = random.choice([0.01,0.1,1.0])\n",
    "    lr_decay_steps = random.choice([100,200,300])\n",
    "    lr_decay_rate = random.choice([0.9,0.95,0.99])\n",
    "\n",
    "    return {\n",
    "        'D': D,\n",
    "        'Dp': Dp,\n",
    "        'nn_hidden_layer_dims': nn_hidden_layer_dims,\n",
    "        'batch_size': batch_size,\n",
    "        'n_samples': n_samples,\n",
    "        'pZ_prior_stddev': pZ_prior_stddev,\n",
    "        'pR_stddev': pR_stddev,\n",
    "        'nn_W_init_mean': nn_W_init_mean,\n",
    "        'nn_W_init_stddev': nn_W_init_stddev,\n",
    "        'nn_b_init_mean': nn_b_init_mean,\n",
    "        'nn_b_init_stddev': nn_b_init_stddev,\n",
    "        'optimizer': optimizer,\n",
    "        'lr_init': lr_init,\n",
    "        'lr_decay_steps': lr_decay_steps,\n",
    "        'lr_decay_rate': lr_decay_rate\n",
    "    }\n",
    "\n",
    "def save_output_csv(fname, data_list):    \n",
    "    import csv\n",
    "    with open(fname, 'w') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow(data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "_SESS = None\n",
    "def get_new_session():\n",
    "    global _SESS\n",
    "    if _SESS is not None:\n",
    "        _SESS.close()\n",
    "    _SESS = tf.Session()\n",
    "    return _SESS\n",
    "    \n",
    "def hypersearch(folder, model_name, dataset_name, seed, n_iter, R, train_mask, valid_mask, verbose=False):\n",
    "    import os\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "\n",
    "    csv_output = \"{}/{}_{}_{}_{}.csv\".format(folder, model_name, dataset_name, seed, n_iter)\n",
    "    mdl_output = \"{}/{}_{}_{}_{}.pkl\".format(folder, model_name, dataset_name, seed, n_iter)\n",
    "    \n",
    "    if os.path.exists(csv_output):\n",
    "        print(\"Skip #{}\".format(seed))\n",
    "    else:\n",
    "        hypers = get_hypers_config(seed)\n",
    "        if verbose: print(hypers)\n",
    "\n",
    "        tf.reset_default_graph()\n",
    "\n",
    "        sess = get_new_session()\n",
    "        with sess.as_default():\n",
    "            model = NNMF(ratings_matrix=R, **hypers)\n",
    "            losses = model.train(train_mask, n_iter=n_iter, verbose=verbose)\n",
    "            if verbose:\n",
    "                plt.plot(losses)\n",
    "                plt.show()\n",
    "\n",
    "            # Evaluation\n",
    "            if verbose: print('Evaluating...')\n",
    "            def get_mse(mask):\n",
    "                # We evaluate 10 draws at a time so everything fits into memory.\n",
    "                results_batches = []\n",
    "                for _ in range(10): # 100 draws total\n",
    "                    idx_i_all, idx_j_all = np.where(mask)\n",
    "                    feed_dict = {\n",
    "                        model.test_idx_i: idx_i_all,\n",
    "                        model.test_idx_j: idx_j_all,\n",
    "                        model.n_test_samples: 10\n",
    "                    }\n",
    "                    results_batch = np.squeeze(sess.run(model.sample_rhats, feed_dict))\n",
    "                    results_batches.append(np.mean(results_batch, axis=0))\n",
    "                results_batches = np.array(results_batches)\n",
    "                results = np.mean(results_batches, axis=0)\n",
    "                mse = np.mean(np.square(results - R[idx_i_all, idx_j_all]))\n",
    "                return mse\n",
    "            \n",
    "            train_mse = get_mse(train_mask)\n",
    "            valid_mse = get_mse(valid_mask)\n",
    "\n",
    "            # Output\n",
    "            data_list = [model_name, dataset_name, seed, n_iter, losses[-1], train_mse, valid_mse]\n",
    "            if verbose: print(data_list)\n",
    "            save_output_csv(csv_output, data_list)\n",
    "            save_graph_parameters(mdl_output)\n",
    "            if verbose: print(\"Done #{}\".format(seed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your data here:\n",
    "import sys \n",
    "sys.path.append('..')\n",
    "\n",
    "# Soon's code provides us with training matrix\n",
    "from sclrecommender.parser import MovieLensParser\n",
    "seedNum = 196\n",
    "np.random.seed(seedNum)\n",
    "dataDirectory = \"../ml-100k\"\n",
    "mlp = MovieLensParser(dataDirectory)\n",
    "R = mlp.getRatingMatrixCopy()\n",
    "\n",
    "# ---\n",
    "import random\n",
    "some_really_random_number = random.randint(0,999999999)\n",
    "# ---\n",
    "\n",
    "mask = R>0\n",
    "\n",
    "np.random.seed(1337) # Set a fixed seed, so we get a fixed mask.\n",
    "TRAIN_MASK = (np.random.binomial(1, 0.9, size=R.shape)) & mask\n",
    "VALID_MASK = (1-TRAIN_MASK) & mask\n",
    "\n",
    "np.random.seed(some_really_random_number) # Go back to \"true\" randomness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "307998201\n",
      "{'D': 56, 'Dp': 80, 'nn_hidden_layer_dims': [5, 30, 49], 'batch_size': 308, 'n_samples': 100, 'pZ_prior_stddev': 1.97, 'pR_stddev': 1.41, 'nn_W_init_mean': 0.0, 'nn_W_init_stddev': 0.36, 'nn_b_init_mean': 0.0, 'nn_b_init_stddev': 0.35, 'optimizer': 'adam', 'lr_init': 1.0, 'lr_decay_steps': 100, 'lr_decay_rate': 0.9}\n",
      " 140/1000 [ 14%] ████                           ETA: 1083s | Loss: 2566672.250"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed eval>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-63-5f7d6ac5b172>\u001b[0m in \u001b[0;36mhypersearch\u001b[0;34m(folder, model_name, dataset_name, seed, n_iter, R, train_mask, valid_mask, verbose)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNNMF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mratings_matrix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mhypers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m             \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m                 \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/eddiedu/csc2541_project/models/nnmf_svi_eddie.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, mask, n_iter, verbose)\u001b[0m\n\u001b[1;32m    356\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mr_ph\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mR_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx_j\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m             }\n\u001b[0;32m--> 358\u001b[0;31m             \u001b[0minfo_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minference\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    359\u001b[0m             \u001b[0minfo_dicts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfo_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/eddiedu/Library/Python/3.6/lib/python/site-packages/edward/inferences/variational_inference.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, feed_dict)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[0msess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mincrement_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/eddiedu/Library/Python/3.6/lib/python/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/eddiedu/Library/Python/3.6/lib/python/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1122\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1124\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1125\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/eddiedu/Library/Python/3.6/lib/python/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1321\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/eddiedu/Library/Python/3.6/lib/python/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/eddiedu/Library/Python/3.6/lib/python/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1304\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1306\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "MODEL_NAME = 'NNMF'\n",
    "DATASET_NAME = 'TEST'\n",
    "SEED = np.random.randint(999999999)\n",
    "N_ITER = 200\n",
    "OUTPUT_FOLDER = 'hypersearch_v1'\n",
    "\n",
    "print(SEED)\n",
    "%time hypersearch(OUTPUT_FOLDER, MODEL_NAME, DATASET_NAME, SEED, N_ITER, R, TRAIN_MASK, VALID_MASK, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
